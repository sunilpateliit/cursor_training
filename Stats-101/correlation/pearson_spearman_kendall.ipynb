{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "28baed1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.figure_factory as ff\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
        "from ydata_profiling import ProfileReport\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotly to work in notebook\n",
        "# import plotly.io as pio\n",
        "# pio.renderers.default = 'notebook'\n",
        "\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"browser\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7c621b81",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iris dataset shape: (150, 6)\n",
            "\n",
            "First 5 rows:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target species  \n",
            "0       0  setosa  \n",
            "1       0  setosa  \n",
            "2       0  setosa  \n",
            "3       0  setosa  \n",
            "4       0  setosa  \n",
            "\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 6 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   sepal length (cm)  150 non-null    float64\n",
            " 1   sepal width (cm)   150 non-null    float64\n",
            " 2   petal length (cm)  150 non-null    float64\n",
            " 3   petal width (cm)   150 non-null    float64\n",
            " 4   target             150 non-null    int64  \n",
            " 5   species            150 non-null    object \n",
            "dtypes: float64(4), int64(1), object(1)\n",
            "memory usage: 7.2+ KB\n",
            "None\n",
            "\n",
            "Target distribution:\n",
            "species\n",
            "setosa        50\n",
            "versicolor    50\n",
            "virginica     50\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 1) Load iris dataset\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "\n",
        "print(\"Iris dataset shape:\", df.shape)\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset info:\")\n",
        "print(df.info())\n",
        "print(\"\\nTarget distribution:\")\n",
        "print(df['species'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ff7eef8d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 120410.64it/s]0:00, 235.26it/s, Describe variable: species] \n",
            "Summarize dataset: 100%|██████████| 34/34 [00:00<00:00, 67.81it/s, Completed]                                    \n",
            "Generate report structure: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
            "Render HTML: 100%|██████████| 1/1 [00:00<00:00, 31.45it/s]\n",
            "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 89.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pandas profiling report saved as 'iris_dataset_profile.html'\n",
            "Note: Profile report saved to file. Open iris_dataset_profile.html to view the report.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 1.1) Generate pandas-profile-report and save as HTML\n",
        "try:\n",
        "    from ydata_profiling import ProfileReport\n",
        "    profile = ProfileReport(df, title=\"Iris Dataset Profiling Report\", explorative=True, correlations={\"pearson\": {\"calculate\": True}, \"spearman\": {\"calculate\": True}, \"kendall\": {\"calculate\": True}})\n",
        "    profile.to_file(\"iris_dataset_profile.html\")\n",
        "    print(\"Pandas profiling report saved as 'iris_dataset_profile.html'\")\n",
        "    print(\"Note: Profile report saved to file. Open iris_dataset_profile.html to view the report.\")\n",
        "    # Prevent automatic display that causes nbformat issues\n",
        "    del profile\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not generate profile report due to: {e}\")\n",
        "    print(\"Continuing with analysis...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "66a29571",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model trained successfully!\n",
            "Training accuracy: 1.0000\n",
            "Test accuracy: 0.9333\n",
            "PR AUC for setosa: 1.0000\n",
            "PR AUC for versicolor: 0.9810\n",
            "PR AUC for virginica: 0.9770\n",
            "Macro-averaged PR AUC: 0.9860\n"
          ]
        }
      ],
      "source": [
        "# # 2) Fit a GBDT model on the raw data\n",
        "# # Prepare data\n",
        "# X = df.drop(['target', 'species'], axis=1)\n",
        "# y = df['target']\n",
        "\n",
        "# # Split the data\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# # Fit GBDT model (using XGBoost for better performance)\n",
        "# gbdt_model = xgb.XGBClassifier(\n",
        "#     n_estimators=100,\n",
        "#     max_depth=3,\n",
        "#     learning_rate=0.1,\n",
        "#     random_state=42,\n",
        "#     eval_metric='mlogloss'\n",
        "# )\n",
        "\n",
        "# gbdt_model.fit(X_train, y_train)\n",
        "\n",
        "# # Make predictions\n",
        "# y_pred = gbdt_model.predict(X_test)\n",
        "\n",
        "# print(\"Model trained successfully!\")\n",
        "# print(f\"Training accuracy: {gbdt_model.score(X_train, y_train):.4f}\")\n",
        "# print(f\"Test accuracy: {gbdt_model.score(X_test, y_test):.4f}\")\n",
        "\n",
        "def train_model_iris(df: pd.DataFrame):\n",
        "    from sklearn.metrics import precision_recall_curve, auc\n",
        "    # 2) Fit a GBDT model on the raw data\n",
        "    # Prepare data\n",
        "    X = df.drop(['target', 'species'], axis=1)\n",
        "    y = df['target']\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    # Fit GBDT model (using XGBoost for better performance)\n",
        "    gbdt_model = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=3,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        eval_metric='mlogloss'\n",
        "    )\n",
        "\n",
        "    gbdt_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = gbdt_model.predict(X_test)\n",
        "    y_pred_proba = gbdt_model.predict_proba(X_test)\n",
        "\n",
        "    print(\"Model trained successfully!\")\n",
        "    print(f\"Training accuracy: {gbdt_model.score(X_train, y_train):.4f}\")\n",
        "    print(f\"Test accuracy: {gbdt_model.score(X_test, y_test):.4f}\")\n",
        "\n",
        "    # Calculate PR AUC for each class\n",
        "    pr_auc_scores = []\n",
        "    for i, class_name in enumerate(['setosa', 'versicolor', 'virginica']):\n",
        "        precision, recall, _ = precision_recall_curve(y_test == i, y_pred_proba[:, i])\n",
        "        pr_auc = auc(recall, precision)\n",
        "        pr_auc_scores.append(pr_auc)\n",
        "        print(f\"PR AUC for {class_name}: {pr_auc:.4f}\")\n",
        "\n",
        "    # Calculate macro average\n",
        "    macro_pr_auc = sum(pr_auc_scores) / len(pr_auc_scores)\n",
        "    print(f\"Macro-averaged PR AUC: {macro_pr_auc:.4f}\")\n",
        "    return gbdt_model\n",
        "\n",
        "gbdt_model = train_model_iris(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "342bb2b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suppress ProfileReport display to avoid nbformat issues\n",
        "# The report has been saved to iris_dataset_profile.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "61df515d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Plot the classification report as heatmap using plotly\n",
        "# Get classification report as dictionary\n",
        "report_dict = classification_report(y_test, y_pred, target_names=iris.target_names, output_dict=True)\n",
        "\n",
        "# Convert to DataFrame for easier plotting\n",
        "report_df = pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "# Create heatmap data\n",
        "metrics = ['precision', 'recall', 'f1-score', 'support']\n",
        "classes = iris.target_names.tolist() + ['accuracy', 'macro avg', 'weighted avg']\n",
        "\n",
        "# Create the heatmap\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "    z=report_df[metrics].values,\n",
        "    x=metrics,\n",
        "    y=classes,\n",
        "    colorscale='RdYlBu_r',\n",
        "    text=np.round(report_df[metrics].values, 3),\n",
        "    texttemplate='%{text}',\n",
        "    textfont={\"size\": 12},\n",
        "    hoverongaps=False\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Classification Report Heatmap',\n",
        "    xaxis_title='Metrics',\n",
        "    yaxis_title='Classes',\n",
        "    width=800,\n",
        "    height=500,\n",
        "    font=dict(size=14)\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "cde57bf1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Importance:\n",
            "             feature  importance\n",
            "3   petal width (cm)    0.492275\n",
            "2  petal length (cm)    0.469418\n",
            "0  sepal length (cm)    0.025505\n",
            "1   sepal width (cm)    0.012802\n"
          ]
        }
      ],
      "source": [
        "# 4) Get the feature importance\n",
        "feature_importance = gbdt_model.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "# Create DataFrame for feature importance\n",
        "feature_imp_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Feature Importance:\")\n",
        "print(feature_imp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "34c0ae30",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Importance with Cumulative Gain:\n",
            "             feature  importance  cumulative_importance  cumulative_percentage\n",
            "3   petal width (cm)    0.492275               0.492275              49.227474\n",
            "2  petal length (cm)    0.469418               0.961693              96.169312\n",
            "0  sepal length (cm)    0.025505               0.987198              98.719826\n",
            "1   sepal width (cm)    0.012802               1.000000             100.000000\n"
          ]
        }
      ],
      "source": [
        "# 5) Get the cumulative feature importance gain with features sorted in reverse of their feature importance\n",
        "# Features are already sorted in descending order of importance\n",
        "feature_imp_df['cumulative_importance'] = feature_imp_df['importance'].cumsum()\n",
        "feature_imp_df['cumulative_percentage'] = feature_imp_df['cumulative_importance'] / feature_imp_df['importance'].sum() * 100\n",
        "\n",
        "print(\"Feature Importance with Cumulative Gain:\")\n",
        "print(feature_imp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "635289da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) & 7) Plot the cumulative feature importance gain with features sorted in reverse of their feature importance\n",
        "# Add markers at 80% cumulative gain\n",
        "\n",
        "# Find the 80% threshold\n",
        "threshold_80 = 80.0\n",
        "idx_80 = np.where(feature_imp_df['cumulative_percentage'] >= threshold_80)[0][0]\n",
        "\n",
        "# Create the plot\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add cumulative importance line\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=list(range(len(feature_imp_df))),\n",
        "    y=feature_imp_df['cumulative_percentage'],\n",
        "    mode='lines+markers',\n",
        "    name='Cumulative Importance',\n",
        "    line=dict(color='blue', width=3),\n",
        "    marker=dict(size=8, color='blue'),\n",
        "    hovertemplate='Feature: %{customdata}<br>Cumulative: %{y:.2f}%<extra></extra>',\n",
        "    customdata=feature_imp_df['feature']\n",
        "))\n",
        "\n",
        "# Add 80% threshold line\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[0, len(feature_imp_df)-1],\n",
        "    y=[80, 80],\n",
        "    mode='lines',\n",
        "    name='80% Threshold',\n",
        "    line=dict(color='red', width=2, dash='dash'),\n",
        "    hovertemplate='80% Threshold<extra></extra>'\n",
        "))\n",
        "\n",
        "# Add marker at 80% point\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[idx_80],\n",
        "    y=[feature_imp_df.iloc[idx_80]['cumulative_percentage']],\n",
        "    mode='markers',\n",
        "    name='80% Mark',\n",
        "    marker=dict(size=15, color='red', symbol='diamond'),\n",
        "    hovertemplate='80%% Mark<br>Feature: %{customdata}<br>Cumulative: %{y:.2f}%<extra></extra>',\n",
        "    customdata=[feature_imp_df.iloc[idx_80]['feature']]\n",
        "))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='Cumulative Feature Importance Gain',\n",
        "    xaxis_title='Number of Features',\n",
        "    yaxis_title='Cumulative Importance (%)',\n",
        "    width=900,\n",
        "    height=600,\n",
        "    font=dict(size=14),\n",
        "    showlegend=True,\n",
        "    xaxis=dict(tickmode='array', tickvals=list(range(len(feature_imp_df))), ticktext=feature_imp_df['feature'])\n",
        ")\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "fig.update_xaxes(tickangle=45)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4e1ba150",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features after 80% cumulative importance mark (2 features):\n",
            "             feature  importance  cumulative_percentage\n",
            "0  sepal length (cm)    0.025505              98.719826\n",
            "1   sepal width (cm)    0.012802             100.000000\n"
          ]
        }
      ],
      "source": [
        "# 8) Get all features after the above mark\n",
        "less_important_features = feature_imp_df.iloc[idx_80 + 1:]\n",
        "\n",
        "print(f\"Features after 80% cumulative importance mark ({idx_80 + 1} features):\")\n",
        "print(less_important_features[['feature', 'importance', 'cumulative_percentage']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ac4ba99e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9) Display beautifully the less important features using plotly in a good white canvas and in tabular form\n",
        "# Calculate statistics for less important features\n",
        "stats_data = []\n",
        "total_rows = len(df)\n",
        "for feature in less_important_features['feature']:\n",
        "    feature_data = df[feature]\n",
        "    non_null_count = feature_data.count()\n",
        "    null_count = total_rows - non_null_count\n",
        "    null_percentage = (null_count / total_rows) * 100 if total_rows > 0 else 0\n",
        "    stats = {\n",
        "        'Feature Name': feature,\n",
        "        'Feature Importance': f\"{less_important_features[less_important_features['feature'] == feature]['importance'].values[0]:.4f}\",\n",
        "        'Non-null Values': non_null_count,\n",
        "        'Null Values %': f\"{null_percentage:.2f}%\",\n",
        "        'Mean': f\"{feature_data.mean():.4f}\",\n",
        "        'Standard Deviation': f\"{feature_data.std():.4f}\",\n",
        "        'Range': f\"{feature_data.max() - feature_data.min():.4f}\",\n",
        "        'Median': f\"{feature_data.median():.4f}\",\n",
        "        'IQR': f\"{feature_data.quantile(0.75) - feature_data.quantile(0.25):.4f}\",\n",
        "        'Q1': f\"{feature_data.quantile(0.25):.4f}\",\n",
        "        'Q3': f\"{feature_data.quantile(0.75):.4f}\"\n",
        "    }\n",
        "    stats_data.append(stats)\n",
        "\n",
        "# Create DataFrame for the table\n",
        "stats_df = pd.DataFrame(stats_data)\n",
        "\n",
        "# Create the table using plotly\n",
        "fig = go.Figure(data=[go.Table(\n",
        "    columnwidth=[200, 150, 120, 120, 100, 150, 100, 100, 100, 100, 100],\n",
        "    header=dict(\n",
        "        values=['Feature Name', 'Feature Importance', 'Non-null Values', 'Null Values %', 'Mean', 'Standard Deviation', 'Range', 'Median', 'IQR', 'Q1', 'Q3'],\n",
        "        fill_color='lightblue',\n",
        "        align='center',\n",
        "        font=dict(size=12, color='black'),\n",
        "        height=40\n",
        "    ),\n",
        "    cells=dict(\n",
        "        values=[stats_df[col] for col in stats_df.columns],\n",
        "        fill_color='white',\n",
        "        align=['left', 'center', 'center', 'center', 'center', 'center', 'center', 'center', 'center', 'center', 'center'],\n",
        "        font=dict(size=11, color='black'),\n",
        "        height=35,\n",
        "        line_color='lightgray'\n",
        "    )\n",
        ")])\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text='Less Important Features Statistics<br><sup>Features contributing after 80% cumulative importance threshold</sup>',\n",
        "        x=0.5,\n",
        "        y=0.95,\n",
        "        xanchor='center',\n",
        "        yanchor='top',\n",
        "        font=dict(size=16, color='black')\n",
        "    ),\n",
        "    width=1200,\n",
        "    height=400,\n",
        "    margin=dict(l=20, r=20, t=80, b=20),\n",
        "    paper_bgcolor='white',\n",
        "    plot_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "5dda7f18",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "def test_less_features(features_to_drop: List[str]):\n",
        "    from sklearn.metrics import precision_recall_curve, auc\n",
        "    # 2) Fit a GBDT model on the raw data\n",
        "    # Prepare data\n",
        "    X = df.drop(['target', 'species'] + features_to_drop, axis=1)\n",
        "    y = df['target']\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    # Fit GBDT model (using XGBoost for better performance)\n",
        "    gbdt_model = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=3,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        eval_metric='mlogloss'\n",
        "    )\n",
        "\n",
        "    gbdt_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = gbdt_model.predict(X_test)\n",
        "    y_pred_proba = gbdt_model.predict_proba(X_test)\n",
        "\n",
        "    print(\"Model trained successfully!\")\n",
        "    print(f\"Training accuracy: {gbdt_model.score(X_train, y_train):.4f}\")\n",
        "    print(f\"Test accuracy: {gbdt_model.score(X_test, y_test):.4f}\")\n",
        "\n",
        "    # Calculate PR AUC for each class\n",
        "    pr_auc_scores = []\n",
        "    for i, class_name in enumerate(['setosa', 'versicolor', 'virginica']):\n",
        "        precision, recall, _ = precision_recall_curve(y_test == i, y_pred_proba[:, i])\n",
        "        pr_auc = auc(recall, precision)\n",
        "        pr_auc_scores.append(pr_auc)\n",
        "        print(f\"PR AUC for {class_name}: {pr_auc:.4f}\")\n",
        "\n",
        "    # Calculate macro average\n",
        "    macro_pr_auc = sum(pr_auc_scores) / len(pr_auc_scores)\n",
        "    print(f\"Macro-averaged PR AUC: {macro_pr_auc:.4f}\")\n",
        "\n",
        "    return gbdt_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "e44ac4aa",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n",
              "       'petal width (cm)', 'target', 'species'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "b2099de4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model trained successfully!\n",
            "Training accuracy: 0.9810\n",
            "Test accuracy: 0.9556\n",
            "PR AUC for setosa: 1.0000\n",
            "PR AUC for versicolor: 0.9841\n",
            "PR AUC for virginica: 0.9827\n",
            "Macro-averaged PR AUC: 0.9889\n"
          ]
        }
      ],
      "source": [
        "\n",
        "new_model = test_less_features(features_to_drop=['sepal length (cm)', 'sepal width (cm)'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "fcac3218",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Detailed Results:\n"
          ]
        }
      ],
      "source": [
        "def compare_models_with_metrics():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score, f1_score, classification_report\n",
        "    import plotly.graph_objects as go\n",
        "    \n",
        "    # Define different feature sets to compare\n",
        "    feature_sets = {\n",
        "        'All Features': [],\n",
        "        'Drop Sepal Features': ['sepal length (cm)', 'sepal width (cm)'],\n",
        "        'Drop Petal Features': ['petal length (cm)', 'petal width (cm)'],\n",
        "        'Only Sepal Length': ['sepal width (cm)', 'petal length (cm)', 'petal width (cm)'],\n",
        "        'Only Petal Features': ['sepal length (cm)', 'sepal width (cm)']\n",
        "    }\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for name, features_to_drop in feature_sets.items():\n",
        "        # Prepare data\n",
        "        X = df.drop(['target', 'species'] + features_to_drop, axis=1)\n",
        "        y = df['target']\n",
        "        \n",
        "        # Split the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "        \n",
        "        # Fit GBDT model\n",
        "        gbdt_model = xgb.XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=3,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42,\n",
        "            eval_metric='mlogloss'\n",
        "        )\n",
        "        \n",
        "        gbdt_model.fit(X_train, y_train)\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = gbdt_model.predict(X_test)\n",
        "        y_pred_proba = gbdt_model.predict_proba(X_test)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        report = classification_report(y_test, y_pred, target_names=['setosa', 'versicolor', 'virginica'], output_dict=True)\n",
        "        \n",
        "        # Calculate PR AUC for each class\n",
        "        pr_auc_scores = []\n",
        "        for i in range(3):\n",
        "            precision, recall, _ = precision_recall_curve(y_test == i, y_pred_proba[:, i])\n",
        "            pr_auc = auc(recall, precision)\n",
        "            pr_auc_scores.append(pr_auc)\n",
        "        \n",
        "        macro_pr_auc = sum(pr_auc_scores) / len(pr_auc_scores)\n",
        "        \n",
        "        results.append({\n",
        "            'Model': name,\n",
        "            'Features Used': list(X.columns),\n",
        "            'PR AUC': macro_pr_auc,\n",
        "            'Precision': report['macro avg']['precision'],\n",
        "            'Recall': report['macro avg']['recall'],\n",
        "            'F1 Score': report['macro avg']['f1-score'],\n",
        "            'Accuracy': report['accuracy']\n",
        "        })\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Calculate percentage changes from baseline (All Features)\n",
        "    baseline = results_df[results_df['Model'] == 'All Features'].iloc[0]\n",
        "    \n",
        "    for metric in ['PR AUC', 'Precision', 'Recall', 'F1 Score', 'Accuracy']:\n",
        "        results_df[f'{metric} % Change'] = ((results_df[metric] - baseline[metric]) / baseline[metric] * 100).round(2)\n",
        "    \n",
        "    # Create color-coded visualization\n",
        "    metrics_to_plot = ['PR AUC', 'Precision', 'Recall', 'F1 Score']\n",
        "    \n",
        "    from plotly.subplots import make_subplots\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=['PR AUC % Change', 'Precision % Change', 'Recall % Change', 'F1 Score % Change'],\n",
        "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
        "               [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
        "    )\n",
        "    \n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        values = results_df[f'{metric} % Change']\n",
        "        \n",
        "        # Create color scale based on values\n",
        "        colors = []\n",
        "        for val in values:\n",
        "            if val > 0:\n",
        "                # Green shades for positive changes\n",
        "                intensity = min(abs(val) / 20, 1)  # Max intensity at 20% change\n",
        "                colors.append(f'rgba(0, {int(128 + intensity * 127)}, 0, 0.8)')\n",
        "            else:\n",
        "                # Red shades for negative changes\n",
        "                intensity = min(abs(val) / 20, 1)  # Max intensity at 20% change\n",
        "                colors.append(f'rgba({int(128 + intensity * 127)}, 0, 0, 0.8)')\n",
        "        \n",
        "        row = i // 2 + 1\n",
        "        col = i % 2 + 1\n",
        "        \n",
        "        fig.add_trace(go.Bar(\n",
        "            x=results_df['Model'],\n",
        "            y=values,\n",
        "            name=metric,\n",
        "            marker_color=colors,\n",
        "            text=[f'{val:+.2f}%' for val in values],\n",
        "            textposition='auto',\n",
        "            showlegend=False\n",
        "        ), row=row, col=col)\n",
        "    \n",
        "    # Update layout for subplots\n",
        "    fig.update_layout(\n",
        "        title='Model Performance Comparison: % Change from Baseline (All Features)',\n",
        "        height=800,\n",
        "        showlegend=False\n",
        "    )\n",
        "    \n",
        "    # Update axis labels\n",
        "    for i in range(4):\n",
        "        row = i // 2 + 1\n",
        "        col = i % 2 + 1\n",
        "        fig.update_xaxes(title_text='Model Configuration', tickangle=45, row=row, col=col)\n",
        "        fig.update_yaxes(title_text='% Change', row=row, col=col)\n",
        "    \n",
        "    fig.show()\n",
        "    \n",
        "    # Also display the results table\n",
        "    print(\"\\nDetailed Results:\")\n",
        "    display_cols = ['Model', 'Features Used', 'PR AUC', 'Precision', 'Recall', 'F1 Score', 'Accuracy'] + \\\n",
        "                   [f'{metric} % Change' for metric in ['PR AUC', 'Precision', 'Recall', 'F1 Score', 'Accuracy']]\n",
        "    \n",
        "    # Create a formatted table using plotly\n",
        "    table_fig = go.Figure(data=[go.Table(\n",
        "        columnwidth=[150, 200, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80],\n",
        "        header=dict(\n",
        "            values=display_cols,\n",
        "            fill_color='lightblue',\n",
        "            align='center',\n",
        "            font=dict(size=10, color='black'),\n",
        "            height=40\n",
        "        ),\n",
        "        cells=dict(\n",
        "            values=[results_df[col].apply(lambda x: ', '.join(x) if isinstance(x, list) else f'{x:.4f}' if isinstance(x, float) else str(x)) for col in display_cols],\n",
        "            fill_color='white',\n",
        "            align=['left', 'left', 'center', 'center', 'center', 'center', 'center', 'center', 'center', 'center', 'center', 'center'],\n",
        "            font=dict(size=9, color='black'),\n",
        "            height=30\n",
        "        )\n",
        "    )])\n",
        "    \n",
        "    table_fig.update_layout(\n",
        "        title='Detailed Model Comparison Results',\n",
        "        width=1400,\n",
        "        height=400,\n",
        "        margin=dict(l=10, r=10, t=50, b=10)\n",
        "    )\n",
        "    \n",
        "    table_fig.show()\n",
        "    \n",
        "    return results_df\n",
        "\n",
        "# Run the comparison\n",
        "comparison_results = compare_models_with_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30c0073d",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
